{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dknHsEosWM16"
   },
   "source": [
    "# Standard Parallelism in C++\n",
    "\n",
    "**Author**: [Debabrata Mandal](https://www.linkedin.com/in/debabrata-mandal/)\n",
    "\n",
    "Copyright (C) 2022 Debabrata Mandal \n",
    "\n",
    "LICENSE: MIT License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgbDSyPPU3_8"
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. <a href=\"#\"> Introduction </a>\n",
    "2. <a href=\"#\"> Setup </a>\n",
    "3. <a href=\"#\"> Examine CUDA C++ code </a>\n",
    "4. <a href=\"#\"> Execution policies (since C++17) </a>\n",
    "5. <a href=\"#\"> Matrix multiplication using std::par </a>\n",
    "6. <a href=\"#\"> Conclusion </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODWDm9v9USMb"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to the first assignment, on heterogenous computing in C++ using [standard parallelism](https://www.nvidia.com/en-us/on-demand/session/gtcspring22-s41960/). Since C++17 there has been an easier way to write parallel code targeted for different kinds of platforms (e.g. CPU and GPUs). This was added to the standard under the term of **Execution policies** and many popular algorithms ([std::sort](https://en.cppreference.com/w/cpp/algorithm/sort)) already support this out of the box. \n",
    "\n",
    "\n",
    "In this assignment, we explore what standard parallism means and what it implies for users of the language. It is not compulsory but recommended that readers refer to all the linked articles and videos scattered throughout this notebook.\n",
    "\n",
    "**Important**\n",
    "\n",
    "* Total points for this assignment: 39 <br>\n",
    "    Bonus Points: 5\n",
    "\n",
    "* Submission deadline: 23:59 Sunday July 24th, 2022.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b> NOTE: </b> <span style=\"color:black\"> <b> Since some parts of this assignment will be automatically graded, refrain from adding or deleting any cells in this notebook. Answer cells have been already provided for the respective questions. You will need to submit the modified juypter notebook preferebly with the individual cell outputs.</b> </span> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6woeNFPkb2b7"
   },
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxTgsTQVcFsM"
   },
   "source": [
    "This assignment will explore the usage of GPUs you will have to install the necessary tools (compiler and profiler) to successfully complete it. \n",
    "\n",
    "Specifically we will need the following tools:\n",
    "\n",
    "1. [nvc++](https://www.youtube.com/watch?v=KhZvrF_w1ak) compiler (part of the [Nvidia HPC SDK](https://developer.nvidia.com/hpc-sdk))\n",
    "2. [NSIGHT Systems](https://developer.nvidia.com/nsight-systems) (nsys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdwX8KGGtV30"
   },
   "source": [
    "The notebook has been primarily designed for readers not having access to a GPU (other than a hosted runtime environment like Colab), we will assume the OS and hardware resources used to solve the questions are similar to the colab version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufTx_gw6UN7y"
   },
   "source": [
    "## Switching between runtimes\n",
    "\n",
    "In order to turn on GPU runtime, \n",
    "- Runtime > Change runtime type > Hardware accelerator > GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcDmhVomUOev"
   },
   "outputs": [],
   "source": [
    "# check GPU is recognised and available\n",
    "! /opt/bin/nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53lMhvE0iSEt"
   },
   "outputs": [],
   "source": [
    "# Make sure we install the right package depending on the OS release.\n",
    "! cat /etc/os-release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0O3HDVAfv50"
   },
   "source": [
    "Now, based on the CUDA toolkit version installed and the OS release find the compatible `nvhpc` package from [this](https://developer.nvidia.com/nvidia-hpc-sdk-releases) list. **(Points: 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNmNvj-Oj6P-"
   },
   "outputs": [],
   "source": [
    "! echo 'deb [trusted=yes] https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64 /' > /etc/apt/sources.list.d/nvhpc.list\n",
    "! sudo apt-get update -y\n",
    "\n",
    "# Replace X,Y & Z with package version.\n",
    "! sudo apt-get install -y nvhpc-XY-Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nIEPQBpik4Ts"
   },
   "outputs": [],
   "source": [
    "# Check compiler version\n",
    "\n",
    "# Replace X,Y & Z with package version.\n",
    "! /opt/nvidia/hpc_sdk/Linux_x86_64/XY.Z/compilers/bin/nvc++ --version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzWloK7ililm"
   },
   "source": [
    "Since we will be using `nvc++` a lot, we should modify the system `PATH` variable to look in the correct locations for `nvc++` rather than us provide it each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2eHg5xco73k"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\"> Warning: If the steps in the next cell, are not followed correctly you might need to restart the runtime.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZUpoDjVlw2_"
   },
   "outputs": [],
   "source": [
    "!echo $PATH # note this path down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BO7KFVjGoi1c"
   },
   "outputs": [],
   "source": [
    "# **Note** the <old $PATH> in command below. \n",
    "# Fill it with the PATH output from the previous cell.\n",
    "# When this notebook was last run it was the following:\n",
    "# /opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n",
    "\n",
    "import os\n",
    "\n",
    "# Replace X,Y & Z with package version.\n",
    "os.environ['PATH']='/opt/nvidia/hpc_sdk/Linux_x86_64/XY.Z/compilers/bin/:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin'\n",
    "\n",
    "!echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDeVVz2oqHxc"
   },
   "outputs": [],
   "source": [
    "!nvc++ --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ks1XV4sanU5n"
   },
   "source": [
    "Find the `nsys` version installed. **(Points: 1)**\n",
    "\n",
    "(Hint: `nsys` is already a part of the `nvhpc` package installed before!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmaLKWQinyXL"
   },
   "outputs": [],
   "source": [
    "# find and display profiler versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0NPZqF5ty7M"
   },
   "source": [
    "Your answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "po6HfUzS4_fv"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOVnWHZRU00L"
   },
   "source": [
    "Curious readers can try exploring the various options [nsys](https://docs.nvidia.com/nsight-systems/UserGuide/index.html) provides. This can prove to be a useful exercise for later parts of this assignment.\n",
    "\n",
    "Some highlighted options are:\n",
    "\n",
    "**nsys**\n",
    "  * --trace\n",
    "  * --stats\n",
    "  * --cuda-memory-usage\n",
    "  * --cuda-um-cpu-page-faults\n",
    "  * --cuda-um-gpu-page-faults\n",
    "  * --gpu-metrics-device\n",
    "\n",
    "To explore the full set of options that `nsys` provides either refer to the official Nvidia documentation on the tools, or simply print them using `--help`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66qQRsNGh3ep"
   },
   "source": [
    "# Examine CUDA C++ code\n",
    "\n",
    "In this part of the assignment, you shall look at 2 CUDA kernels related to [matrix multiplication](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#mat-mat-multi) on the GPU. While this section will not directly make you write code in CUDA, it will set the background for our future discussions.\n",
    "\n",
    "The purpose of this section is to stress on the importance of reading and understanding CUDA code and translating them to platform agnostic code using standard parallelism in later sections.\n",
    "\n",
    "For the code snippet below, add **single** line comments wherever instigated. Be as specific as possible to score the maximum points!**(Points: 3)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjjG_QcipoKZ"
   },
   "source": [
    "```c++\n",
    "template <typename T>\n",
    "__global__ void gpu_gemm_nn(int m, int n, int k,                        //in: matrix dimensions: C(m,n)+=A(m,k)*B(k,n)\n",
    "\t\t\t\t\t\t\tT * __restrict__ dest,                      //inout: pointer to C matrix data\n",
    "\t\t\t\t\t\t\tconst T * __restrict__ left,                //in: pointer to A matrix data\n",
    "\t\t\t\t\t\t\tconst T * __restrict__ right)               //in: pointer to B matrix data\n",
    "{\n",
    "    // <what does ty refer to?>\n",
    "    size_t ty = blockIdx.y*blockDim.y + threadIdx.y; \n",
    "    // <what does tx refer to?>\n",
    "\tsize_t tx = blockIdx.x*blockDim.x + threadIdx.x;\n",
    "    size_t n_pos = ty; \n",
    "    // <why is the following loop necessary?> \n",
    "\twhile(n_pos < n){\n",
    "\t\tsize_t m_pos = tx; \n",
    "\t\twhile(m_pos < m) {\n",
    "            // <what does tmp store?>\n",
    "\t\t\tT tmp = static_cast<T>(0.0);\n",
    "            // <the following loops over which dimension (x or y) of the matrix A and which dimension of matrix B?>\n",
    "\t\t\tfor(size_t k_pos = 0; k_pos < k; ++k_pos)\n",
    "\t\t\t{\n",
    "\t\t\t\ttmp += left[m_pos*k + k_pos] * right[k_pos*n + n_pos];\n",
    "\t\t\t}\n",
    "\t\t\tdest[m_pos*n + n_pos] += tmp;\n",
    "\t\t\tm_pos += gridDim.x*blockDim.x; \n",
    "\t\t}\n",
    "\t\tn_pos += gridDim.y*blockDim.y; \n",
    "\t}\n",
    "\treturn;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cfrOkQgyREg"
   },
   "source": [
    "Your answer:\n",
    "\n",
    "1.\n",
    "\n",
    "2.\n",
    "\n",
    "3.\n",
    "\n",
    "4.\n",
    "\n",
    "5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1rmFFQX5MVo"
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6C3b5bbEsQtP"
   },
   "source": [
    "Work out the arithmetic intensity of matrix multiplication based on the algorithm above. **(Points: 1)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1j2aR9Vs0gh"
   },
   "source": [
    "Your answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCS3vZo75OUg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiMr_uOlu667"
   },
   "source": [
    "Now, look into the file `naive-matmul.cu`. It contains the same kernel that we discussed in the previous cell. Read the `main` function provided and find the execution time for the following workloads. **(Points: 2)**\n",
    "\n",
    "| Workload  |  Execution times (ms) |\n",
    "|---|---|\n",
    "| M=N=K=256  |   |\n",
    "|  M=N=K=512 |   |\n",
    "| M=N=1024 and K=256  |   |\n",
    "|  M=N=K=2048 |   |\n",
    "\n",
    "Attach the profiler screenshots zooming into the timeline **(Points: 2)**\n",
    "1. When the kernel was started - mark/highlight the memcpy operation from Host to Device\n",
    "2. When the kernel finished executing - mark/highlight the memcpy operation from Device to Host\n",
    "\n",
    "The profile can be first generated using `nsys profile` and then visualised using NSIGHT Systems GUI. \n",
    "\n",
    "For the workload M=N=K=1024 find the optimal block size (defined as BLOCK_SIZE in the code). Provide a short explanation for the same. **(Points: 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Soq72147uCMJ"
   },
   "outputs": [],
   "source": [
    "# your code workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ap352Vn0yKnw"
   },
   "source": [
    "Your answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mn24ZyrI5QHe"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSLjW8KX2DRh"
   },
   "source": [
    "**Bonus - Part 1 (2 points)**\n",
    "\n",
    "For this bonus part, continue to make changes to the `naive_matmul.cu` script and measure execution times using it. You will need to submit this modified script.\n",
    "\n",
    "Observe the memory access pattern of the kernel provided and explore variations to find the most efficient one. Show the improved execution times. **(Points: 1)**\n",
    "\n",
    "Try creating pinned memory on the host instead of allocating it using `malloc`. Observe and note the improved execution time. **(Points: 1)**\n",
    "\n",
    "Hint: Look at the matrix initialisation code for A or B inside `main()`. Is A (and B) row major or column major?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lotqmlSxDNW"
   },
   "outputs": [],
   "source": [
    "# your code workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFOQmv_k3D3U"
   },
   "source": [
    "Your answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sILlFdhV5Rs2"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hmj2pSn0_NWF"
   },
   "source": [
    "# Execution policies (since C++17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FOr9r9K_WMm"
   },
   "source": [
    "Since C++17, parallelism was made available for popular `std::` algorithms under the context of execution policies. To read more about this refer to the following articles:\n",
    "1. https://developer.nvidia.com/blog/accelerating-standard-c-with-gpus-using-stdpar/\n",
    "2. https://developer.nvidia.com/blog/developing-accelerated-code-with-standard-language-parallelism/\n",
    "3. https://developer.nvidia.com/blog/multi-gpu-programming-with-standard-parallel-c-part-1/\n",
    "4. https://developer.nvidia.com/blog/multi-gpu-programming-with-standard-parallel-c-part-2/\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Tip:</b> One might find many answers for this assignment hidden in the above links!</div>\n",
    "\n",
    "In this section, we will explore the usage of different execution policies offered by the standard and inspect and try to improve their performance using the `nsys profiler`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rqRPAaGn8yZ"
   },
   "source": [
    "We will first implement some trivial workloads using C++11 (& C++17). These will help us understand the maximum amount of effort needed to make our workloads compatible for GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kh9pLgjoUX3"
   },
   "source": [
    "### [std::sort]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lv5aysOBtWS8"
   },
   "source": [
    "Look inside the file `sort.cpp`. It contains a very simple example which can be our first candidate made eligible for standard parallelism.\n",
    "\n",
    "Your task is to modify the function `naive_sort` to use [`std::execution`](https://en.cppreference.com/w/cpp/algorithm/execution_policy_tag_t) policies, benchmark and find the execution times and fill in the 2 tables below. To find execution times for smaller workloads, make sure you use a high precision clock. **(Points: 3)**\n",
    "\n",
    "(Feel free to modify the script as you please, to automate the benchmarking process.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHI7CsR2wug6"
   },
   "source": [
    "| Time in ms (average over 1000 iterations) | cpu  | multicore  | gpu  |\n",
    "|:---|:---:|:---:|:---:|\n",
    "| no execution policy  |   |   |   |\n",
    "| std::seq  |   |   |   |\n",
    "|  std::unseq |   |   |   |\n",
    "|  std::par |   |   |   |\n",
    "|  std::par_unseq |   |   |   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTTkCpFCwuDw"
   },
   "source": [
    "| Time in ms (average over 1000 iterations)\t  |  N=1024 | N=2048  | N=8192  | N=81920  | N=819200  |   |\n",
    "|:---|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| cpu  |   |   |   |   |   |   |\n",
    "|  multicore |   |   |   |   |   |   |\n",
    "|  gpu |   |   |   |   |   |   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HsxluJMyYdE"
   },
   "source": [
    "Mention atleast one important observation from table 1 and table 2. **(Points: 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15UFcaeCxlDO"
   },
   "outputs": [],
   "source": [
    "# your workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhHjnT6B0QXn"
   },
   "source": [
    "Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Y4UGXZx0R9I"
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwQG7I_80SyY"
   },
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hT82uLNE0ZT-"
   },
   "source": [
    "Look inside the files `sum.cpp`, `vector_product.cpp`, `swaps.cpp` and `increment.cpp`. It contains naive implementations of some popular algorithms. Your task is to examine the code presented and answer the following questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBYpMjPM01hN"
   },
   "source": [
    "Using [`std::for_each`](https://en.cppreference.com/w/cpp/algorithm/for_each) only, for each algorithm mention the execution policy that will perform the best? **(Points: 1)**\n",
    "\n",
    "| Algorithm  | Execution policy used |\n",
    "|---|---|\n",
    "| swaps  |   |\n",
    "|  naive_sum |   |\n",
    "|  naive_increment |   |\n",
    "|  naive_inner_prod |   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IA_hnTtM15CS"
   },
   "source": [
    "Port the above \"naive\" implementations to work on GPUs using the following `std` algorithms. **(Points: 4)**\n",
    "\n",
    "| Algorithm  | std:: | function name |\n",
    "|---|---|---| \n",
    "| swaps  |  for_each | std_swaps |\n",
    "|  naive_sum |  reduce | std_reduce_sum |\n",
    "|  naive_increment | transform | std_transform |\n",
    "|  naive_innerp | inner_product | std_innerp |\n",
    "\n",
    "The functions signatures are already provided in each of the code files. Fill up these empty functions and make sure it works with the `main()` provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cz4X4fIiyl_B"
   },
   "outputs": [],
   "source": [
    "# your workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6wxO4-5-bU3"
   },
   "source": [
    "Based on your implementations, answer the following quetions: **(Points: 4)**\n",
    "1. How well does a parallelised `std_swaps` perform against a sequential implementation?\n",
    "2. On which platform (CPU or GPU) does parallelised `std_reduce_sum` perform better?\n",
    "3. For `N=8192` does parallelised `std_tranform` on GPU ever perform better than sequential implementation? If not, explain why or else mention the scenario when it shows improvement over the sequential implementation. (Hint: Adjust the workload assigned within each loop and observe the behavior.) \n",
    "4. Does `std::inner_product` support execution policies? If not, write a new function `std_transform_reduce` which makes use of an algorithm that supports parallel execution policies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CsCtuluyqDF"
   },
   "source": [
    "Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTrYJUPmyniF"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_oHThwrHYly"
   },
   "source": [
    "### Debugging\n",
    "\n",
    "This section will focus on some common pitfalls when using `std::par` with GPUs. \n",
    "\n",
    "To complete this part, you will need to debug, compile and verify the code snippets provided. In the files `debug*.cpp` make minor changes so that it compiles and the checks work. **(Points: 4)**\n",
    "\n",
    "(Hint: Search for a `nvc++` flag which provides additional debug info during compilation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8qurHBTy25J"
   },
   "outputs": [],
   "source": [
    "# your workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYqp9_lGy4EJ"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAnCkrLGU7bH"
   },
   "source": [
    "# Matrix multiplication using std::par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rlp0qNDNVRCR"
   },
   "source": [
    "Now, you shall finally implement matrix multiplication using std::par. To complete this section, make sure you have the nsys profiler GUI installed and running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6QLl2aTZAHk"
   },
   "source": [
    "**Part 1**\n",
    "\n",
    "The reference script is provided in `matmul_1.cpp`.\n",
    "\n",
    "The points for this part will depend on the relative performance of your optimised implementation in comparison to the CUDA kernels (under a similar workload).\n",
    "\n",
    "**(Total Points: 6)**\n",
    "\n",
    "*Make sure your implementation uses at max 2 nested `std::for_each`/looping blocks for this part.*\n",
    "\n",
    "Q1. What is the relative performance of your unoptimised implementation when compared to the CUDA kernel? **(Points: 0.5)**\n",
    "\n",
    "Q2. In order to reduce the execution time of your implementation, take the help of the `nsys profiler` to collect logs and identify unnecessary `memcpy` cycles from **device to host**. Attach screenshots clearly marking these device to host memcpy operations from the profiler. **(Points: 0.5)**\n",
    "\n",
    "Q3. Refer to [this](https://developer.nvidia.com/blog/multi-gpu-programming-with-standard-parallel-c-part-2/) article to come up with a fix to the script provided. Explain the fix. **(Points: 0.5)**\n",
    "\n",
    "Q4. Based on the finding in the previous question try to optimise your script as much as possible. Note down the new relative performance of your implementation. **(Points: 3.5)**\n",
    "\n",
    "Q5. In the same article linked above, there is yet another optimisation strategy mentioned which further improves the execution time on top of the previous fix. Identify it and mention the relative speed up from step 4. **(Points: 0.5)**\n",
    "\n",
    "Q6. Based on our observation from the CUDA exercise, is `cudaMallocHost` a better way to allocate memory on the host when using standard parallelism? Explain your answer. **(Points: 0.5)**\n",
    "\n",
    "You will need to submit the `matmul_1.cpp` script provided with your (ultra) optimised implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fjlze69SzThs"
   },
   "outputs": [],
   "source": [
    "# your workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5E7WNSfqzQTw"
   },
   "source": [
    "Your answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YD7XuPW8jIJI"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdWau8RsjJ3h"
   },
   "source": [
    "**Part 2 (Bonus)**\n",
    "\n",
    "The reference script is provided in `matmul_2.cpp`.\n",
    "\n",
    "**(Total Points: 3)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gd5vqSE_j588"
   },
   "source": [
    "Q1. In the previous part there was a contraint on the number of nested `std::for_each` blocks that one could use. However, it is more intuitive to represent matrix multiplication in terms of an outer loop on the row index, an intermediate loop on the column index and a inner loop representing the inner product of the row and column. Implement matrix multiplication using atleast 3 nested `std::for_each` blocks. **(Points: 1)**\n",
    "\n",
    "Q2. Did you face any issues in the previous question? What is the best execution time you can obtain using the 3 nested `for_each` blocks? **(Points: 1)**\n",
    "\n",
    "Q3. Can you improve your execution time by switching from row-major to column major for each of the three matrices? How much improvement did you observe? Provide an intuitive explanation for the same. **(Points: 1)**\n",
    "\n",
    "You will need to submit the `matmul_2.cpp` script provided with your nested `for_each` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6toYTThk0HM_"
   },
   "outputs": [],
   "source": [
    "# your workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_V2B3nv0Jmg"
   },
   "source": [
    "Your answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOyYq5mil7i3"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Hopefully, this assignment gave you a fair idea of how convenient it is to represent computational algorithms using standard C++ and leverage hardware accelerators like multicore processors and GPUs with little to no work.\n",
    "\n",
    "**Acknowledgements**\n",
    "\n",
    "* This assignment develops on some of the ideas presented in [this](https://github.com/DmitryLyakh/CUDA_Tutorial) github repository."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
