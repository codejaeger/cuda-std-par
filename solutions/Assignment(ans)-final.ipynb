{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dknHsEosWM16"
      },
      "source": [
        "# Standard Parallelism in C++\n",
        "\n",
        "**Author**: [Debabrata Mandal]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgbDSyPPU3_8"
      },
      "source": [
        "# Table of Contents\n",
        "\n",
        "1. <a href=\"#\"> Introduction </a>\n",
        "2. <a href=\"#\"> Setup </a>\n",
        "3. <a href=\"#\"> Examine CUDA C++ code </a>\n",
        "4. <a href=\"#\"> Execution policies (since C++17) </a>\n",
        "5. <a href=\"#\"> Matrix multiplication using std::par </a>\n",
        "6. <a href=\"#\"> Conclusion </a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODWDm9v9USMb"
      },
      "source": [
        "#Introduction\n",
        "\n",
        "Welcome to the first assignment, on heterogenous computing in C++ using [standard parallelism](https://www.nvidia.com/en-us/on-demand/session/gtcspring22-s41960/). Since C++17 there has been an easier way to write parallel code targeted for different kinds of platforms (e.g. CPU and GPUs). This was added to the standard under the term of **Execution policies** and many popular algorithms ([std::sort](https://en.cppreference.com/w/cpp/algorithm/sort)) already support this out of the box. \n",
        "\n",
        "\n",
        "In this assignment, we explore what standard parallism means and what it implies for users of the language. It is not compulsory but recommended that readers refer to all the linked articles and videos scattered throughout this notebook.\n",
        "\n",
        "<div class=\"alert alert-block alert-info\"> <b> NOTE: </b> <span style=\"color:black\"> <b> Since some parts of this assignment will be automatically graded, refrain from adding or deleting any cells in this notebook. Answer cells have been already provided for the respective questions.</b> </span> </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6woeNFPkb2b7"
      },
      "source": [
        "#Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxTgsTQVcFsM"
      },
      "source": [
        "Since this assignment will explore the usage of GPUs you will have to install the necessary tools (compiler and profiler) to successfully complete it. \n",
        "\n",
        "Specifically we will need the following tools:\n",
        "\n",
        "1. [nvc++](https://www.youtube.com/watch?v=KhZvrF_w1ak) compiler (part of the [Nvidia HPC SDK](https://developer.nvidia.com/hpc-sdk))\n",
        "2. [NSIGHT Systems](https://developer.nvidia.com/nsight-systems) (nsys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdwX8KGGtV30"
      },
      "source": [
        "Since this notebook has been primarily designed for readers not having access to a GPU (other than a hosted runtime environment like Colab), we will assume the OS and hardware resources used to solve the questions are similar to the colab version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufTx_gw6UN7y"
      },
      "source": [
        "## Switching between runtimes\n",
        "\n",
        "In order to turn on GPU runtime, \n",
        "- Runtime > Change runtime type > Hardware accelerator > GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcDmhVomUOev",
        "outputId": "4800bdcc-a17d-463d-967c-d9ba5fc21cf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Aug  5 05:30:29 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# check GPU is recognised and available\n",
        "! /opt/bin/nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53lMhvE0iSEt",
        "outputId": "ce770f5c-9b71-433d-faf3-db566f6de2ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NAME=\"Ubuntu\"\n",
            "VERSION=\"18.04.6 LTS (Bionic Beaver)\"\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "PRETTY_NAME=\"Ubuntu 18.04.6 LTS\"\n",
            "VERSION_ID=\"18.04\"\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "VERSION_CODENAME=bionic\n",
            "UBUNTU_CODENAME=bionic\n"
          ]
        }
      ],
      "source": [
        "# Make sure we install the right package depending on the OS release.\n",
        "! cat /etc/os-release"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0O3HDVAfv50"
      },
      "source": [
        "Now, based on the CUDA toolkit version installed and the OS release find the compatible `nvhpc` package from [this](https://developer.nvidia.com/nvidia-hpc-sdk-releases) list. **(Points: 1)**\n",
        "\n",
        "ANSWER: XY.Z >= 21.2; however nvcc fails to work with the latest package versions due to a PTX compatibility issue. This is because we need a forward compatibility package for the nvcc compiler provided with the latest packages (containing CUDA version >= 11.2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNmNvj-Oj6P-",
        "outputId": "be4f6f87-caf4-4748-d8ad-27389ee9b38e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:6 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,528 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,336 kB]\n",
            "Ign:14 https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64  InRelease\n",
            "Hit:15 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,905 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,306 kB]\n",
            "Ign:19 https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64  Release\n",
            "Get:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [902 kB]\n",
            "Ign:21 https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64  Packages\n",
            "Ign:21 https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64  Packages\n",
            "Ign:21 https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64  Packages\n",
            "Ign:21 https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64  Packages\n",
            "Ign:21 https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64  Packages\n",
            "Ign:21 https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64  Packages\n",
            "Get:21 https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64  Packages [17.9 kB]\n",
            "Fetched 11.3 MB in 6s (1,736 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  nvhpc-2021 nvhpc-21-11\n",
            "The following NEW packages will be installed:\n",
            "  nvhpc-2021 nvhpc-21-11 nvhpc-21-2\n",
            "0 upgraded, 3 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 4,775 MB of archives.\n",
            "After this operation, 15.4 GB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64  nvhpc-21-11 21.11 [2,659 MB]\n",
            "Get:2 https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64  nvhpc-2021 21.11 [1,202 B]\n",
            "Get:3 https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64  nvhpc-21-2 21.2 [2,115 MB]\n",
            "Fetched 4,775 MB in 2min 0s (39.9 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package nvhpc-21-11.\n",
            "(Reading database ... 155680 files and directories currently installed.)\n",
            "Preparing to unpack .../nvhpc-21-11_21.11_amd64.deb ...\n",
            "Unpacking nvhpc-21-11 (21.11) ...\n",
            "Selecting previously unselected package nvhpc-2021.\n",
            "Preparing to unpack .../nvhpc-2021_21.11_amd64.deb ...\n",
            "Unpacking nvhpc-2021 (21.11) ...\n",
            "Selecting previously unselected package nvhpc-21-2.\n",
            "Preparing to unpack .../nvhpc-21-2_21.2_amd64.deb ...\n",
            "Unpacking nvhpc-21-2 (21.2) ...\n",
            "Setting up nvhpc-2021 (21.11) ...\n",
            "Setting up nvhpc-21-11 (21.11) ...\n",
            "Setting up nvhpc-21-2 (21.2) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.5) ...\n"
          ]
        }
      ],
      "source": [
        "! echo 'deb [trusted=yes] https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64 /' > /etc/apt/sources.list.d/nvhpc.list\n",
        "! sudo apt-get update -y\n",
        "\n",
        "# Replace X,Y & Z with package version.\n",
        "! sudo apt-get install -y nvhpc-21-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIEPQBpik4Ts",
        "outputId": "390c26bc-3338-4331-a689-cc6c9a3ed0cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "nvc++ 21.2-0 LLVM 64-bit target on x86-64 Linux -tp haswell \n",
            "NVIDIA Compilers and Tools\n",
            "Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n"
          ]
        }
      ],
      "source": [
        "# Check compiler version\n",
        "\n",
        "# Replace X,Y & Z with package version.\n",
        "! /opt/nvidia/hpc_sdk/Linux_x86_64/21.2/compilers/bin/nvc++ --version "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzWloK7ililm"
      },
      "source": [
        "Since we will be using `nvc++` a lot, we should modify the system `PATH` variable to look in the correct locations for `nvc++` rather than us provide it each time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2eHg5xco73k"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\"> Warning: If the steps in the next cell, are not followed correctly you might need to restart the runtime.</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZUpoDjVlw2_",
        "outputId": "864169c6-7f35-4c77-f7ab-935c2d0e5056"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ],
      "source": [
        "!echo $PATH # note this path down"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BO7KFVjGoi1c",
        "outputId": "5a28eceb-3024-4e6d-f020-12e303ba34a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/opt/nvidia/hpc_sdk/Linux_x86_64/21.2/compilers/bin/:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ],
      "source": [
        "# **Note** the <old $PATH> in command below. \n",
        "# Fill it with the PATH output from the previous cell.\n",
        "# When this notebook was last run it was the following:\n",
        "# /opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n",
        "\n",
        "import os\n",
        "\n",
        "# Replace X,Y & Z with package version.\n",
        "os.environ['PATH']='/opt/nvidia/hpc_sdk/Linux_x86_64/21.2/compilers/bin/:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin'\n",
        "\n",
        "!echo $PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDeVVz2oqHxc",
        "outputId": "686a9954-4145-44e1-b5ee-410dbec9f5b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "nvc++ 21.2-0 LLVM 64-bit target on x86-64 Linux -tp haswell \n",
            "NVIDIA Compilers and Tools\n",
            "Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n"
          ]
        }
      ],
      "source": [
        "!nvc++ --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks1XV4sanU5n"
      },
      "source": [
        "Find the `nsys` version installed. **(Points: 1)**\n",
        "\n",
        "(Hint: `nsys` is already a part of the `nvhpc` package installed before!) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmaLKWQinyXL",
        "outputId": "f79bb843-1d21-40bb-bfa4-4c3a59766b9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NVIDIA Nsight Systems version 2020.5.1.85-5ee086b\n"
          ]
        }
      ],
      "source": [
        "!nsys --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0NPZqF5ty7M"
      },
      "source": [
        "Your answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po6HfUzS4_fv"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOVnWHZRU00L"
      },
      "source": [
        "Curious readers can try exploring the various options [nsys](https://docs.nvidia.com/nsight-systems/UserGuide/index.html) provides. This can prove to be a useful exercise for later parts of this assignment.\n",
        "\n",
        "Some highlighted options are:\n",
        "\n",
        "**nsys**\n",
        "  * --trace\n",
        "  * --stats\n",
        "  * --cuda-memory-usage\n",
        "  * --cuda-um-cpu-page-faults\n",
        "  * --cuda-um-gpu-page-faults\n",
        "  * --gpu-metrics-device\n",
        "\n",
        "To explore the full set of options that `nsys` provides either refer to the official Nvidia documentation on the tools, or simply print them using `--help`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66qQRsNGh3ep"
      },
      "source": [
        "# Examine CUDA C++ code\n",
        "\n",
        "In this part of the assignment, you shall look at 2 CUDA kernels related to [matrix multiplication](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#mat-mat-multi) on the GPU. While this section will not directly make you write code in CUDA, it will set the background for our future discussions.\n",
        "\n",
        "The purpose of this section is to stress on the importance of reading and understanding CUDA code and translating them to platform agnostic code using standard parallelism in later sections.\n",
        "\n",
        "For the code snippet below, add **single** line comments wherever instigated. Be as specific as possible to score the maximum points!**(Points: 3)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjjG_QcipoKZ"
      },
      "source": [
        "```c++\n",
        "template <typename T>\n",
        "__global__ void gpu_gemm_nn(int m, int n, int k,                        //in: matrix dimensions: C(m,n)+=A(m,k)*B(k,n)\n",
        "\t\t\t\t\t\t\tT * __restrict__ dest,                      //inout: pointer to C matrix data\n",
        "\t\t\t\t\t\t\tconst T * __restrict__ left,                //in: pointer to A matrix data\n",
        "\t\t\t\t\t\t\tconst T * __restrict__ right)               //in: pointer to B matrix data\n",
        "{\n",
        "    // <what does ty refer to?>\n",
        "    size_t ty = blockIdx.y*blockDim.y + threadIdx.y; \n",
        "    // <what does tx refer to?>\n",
        "\tsize_t tx = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "    size_t n_pos = ty; \n",
        "    // <why is the following loop necessary?> \n",
        "\twhile(n_pos < n){\n",
        "\t\tsize_t m_pos = tx; \n",
        "\t\twhile(m_pos < m) {\n",
        "            // <what does tmp store?>\n",
        "\t\t\tT tmp = static_cast<T>(0.0);\n",
        "            // <the following loops over which dimension (x or y) of the matrix A and which dimension of matrix B?>\n",
        "\t\t\tfor(size_t k_pos = 0; k_pos < k; ++k_pos)\n",
        "\t\t\t{\n",
        "\t\t\t\ttmp += left[m_pos*k + k_pos] * right[k_pos*n + n_pos];\n",
        "\t\t\t}\n",
        "\t\t\tdest[m_pos*n + n_pos] += tmp;\n",
        "\t\t\tm_pos += gridDim.x*blockDim.x; \n",
        "\t\t}\n",
        "\t\tn_pos += gridDim.y*blockDim.y; \n",
        "\t}\n",
        "\treturn;\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cfrOkQgyREg"
      },
      "source": [
        "Your answer:\n",
        "\n",
        "1. The column id of the cell in the output matrix being computed.\n",
        "\n",
        "2. The row id of the cell in the output matrix being computed.\n",
        "\n",
        "3. If the block dims are not a multiple of the row or the column dimensions of the output matrix some kernel calls from the last blocks along the x and y dimension will be invalid since these blocks are partially contained within the output matrix C. Hence, this check prevents such kernel calls from getting executed to completion.\n",
        "\n",
        "4. this will accumulate the value for an output cell (tx, ty) in the matrix C.\n",
        "\n",
        "5. x dimension of matrix A and y dimension of matrix B."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1rmFFQX5MVo"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C3b5bbEsQtP"
      },
      "source": [
        "Work out the arithnetic intensity of matrix multiplication based on the algorithm above. **(Points: 1)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1j2aR9Vs0gh"
      },
      "source": [
        "Your answer: \n",
        "Refer to https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCS3vZo75OUg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiMr_uOlu667"
      },
      "source": [
        "Now, look into the file `naive-matmul.cu`. It contains the same kernel that we discussed in the previous cell. Read the `main` function provided and find the execution time for the following workloads. **(Points: 2)**\n",
        "\n",
        "| Workload  |  Execution times (ms) |\n",
        "|---|---|\n",
        "| M=N=K=256  |  0.501 |\n",
        "|  M=N=K=512 |  2.442 |\n",
        "| M=N=1024 and K=256  |  4.702 |\n",
        "|  M=N=K=2048 | 147.85  |\n",
        "\n",
        "Attach the profiler screenshots zooming into the timeline **(Points: 2)**\n",
        "1. When the kernel was started - mark/highlight the memcpy operation from Host to Device\n",
        "2. When the kernel finished executing - mark/highlight the memcpy operation from Device to Host\n",
        "\n",
        "The profile can be first generated using `nsys profile` and then visualised using NSIGHT Systems GUI. \n",
        "\n",
        "For the workload M=N=K=1024 find the optimal block size (defined as BLOCK_SIZE in the code). Provide a short explanation for the same. **(Points: 1)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Soq72147uCMJ",
        "outputId": "392f5527-e86c-4302-e2e3-665a8c67fb80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[01m\u001b[Knaive_matmul.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint main()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Knaive_matmul.cu:184:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KcudaError_t cudaThreadSynchronize()\u001b[m\u001b[K’ is deprecated [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     cudaThreadSynchron\u001b[01;35m\u001b[Ki\u001b[m\u001b[Kze();\n",
            "                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/opt/nvidia/hpc_sdk/Linux_x86_64/21.2/cuda/11.2/include/cuda_runtime_api.h:1011:46:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " extern __CUDA_DEPRECATED __host__ cudaError_t\u001b[01;36m\u001b[K CUDARTAPI cudaThread\u001b[m\u001b[KSynchronize(void);\n",
            "                                              \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "Check 1/1\n",
            "Naive matmul 10.0411 ms\n"
          ]
        }
      ],
      "source": [
        "# your code workspace\n",
        "# !/opt/nvidia/hpc_sdk/Linux_x86_64/21.2/compilers/bin/nvcc\n",
        "!nvcc -lnvToolsExt naive_matmul.cu && ./a.out\n",
        "\n",
        "# # For nys profile options refer to this crisp summary of the CLI options\n",
        "# # https://gist.github.com/mcarilli/376821aa1a7182dfcf59928a7cde3223\n",
        "\n",
        "# !nsys profile --trace cuda,nvtx -f true -o report1 ./a.out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap352Vn0yKnw"
      },
      "source": [
        "Your answers:\n",
        "\n",
        "Screenshots\n",
        "\n",
        "1. ![](https://drive.google.com/uc?export=view&id=1VefYQC9RJsFCS7x1ugd778Ph2SeoR9PE)\n",
        "\n",
        "\n",
        "2. ![](https://drive.google.com/uc?export=view&id=1c8KJmhbAA47iYuytb8eJv9Esh4GRzekY)\n",
        "\n",
        "\n",
        "Optimal block size - 8\n",
        "\n",
        "BLOCK SIZE 6 ~ 13 ms\n",
        "\n",
        "BLOCK SIZE 8 ~ 10 ms\n",
        "\n",
        "BLOCK SIZE 10 ~ 13 ms\n",
        "\n",
        "(The exact reason for this is complicated to find due to interaction between block scheduler, thread scheduler, memory address streams generated, internal resource limits, internal buffering mechanisms etc. However for simple kernels like ours it could be possible to find a closed form solution to this. In practical settings, performance tuning will usually reveal the most optimal set of configuration parameters to use.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn24ZyrI5QHe"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSLjW8KX2DRh"
      },
      "source": [
        "**Bonus - Part 1 (2 points)**\n",
        "\n",
        "For this bonus part, continue to make changes to the `naive_matmul.cu` script and measure execution times using it. You will need to submit this modified script.\n",
        "\n",
        "Observe the memory access pattern of the kernel provided and explore variations to find the most efficient one. Show the improved execution times. **(Points: 1)**\n",
        "\n",
        "Try creating pinned memory on the host instead of allocating it using `malloc`. Observe and note the improved execution time. **(Points: 1)**\n",
        "\n",
        "Hint: Look at the matrix initialisation code for A or B inside `main()`. Is A (and B) row major or column major?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lotqmlSxDNW"
      },
      "outputs": [],
      "source": [
        "# your code workspace\n",
        "# Look at the kernel gpu_gemm_nn_optimised naive_matmul.cu file\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFOQmv_k3D3U"
      },
      "source": [
        "Your answer: \n",
        "\n",
        "Changing matrix A to column major improves the performance since individual threads (within a warp) accessing elements along the column leads to better memory access pattern via memory coalescing. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sILlFdhV5Rs2"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hmj2pSn0_NWF"
      },
      "source": [
        "#Execution policies (since C++17)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FOr9r9K_WMm"
      },
      "source": [
        "Since C++17, parallelism was made available for popular `std::` algorithms under the context of execution policies. To read more about this refer to the following articles:\n",
        "1. https://developer.nvidia.com/blog/accelerating-standard-c-with-gpus-using-stdpar/\n",
        "2. https://developer.nvidia.com/blog/developing-accelerated-code-with-standard-language-parallelism/\n",
        "3. https://developer.nvidia.com/blog/multi-gpu-programming-with-standard-parallel-c-part-1/\n",
        "4. https://developer.nvidia.com/blog/multi-gpu-programming-with-standard-parallel-c-part-2/\n",
        "\n",
        "<div class=\"alert alert-block alert-info\"> <b>Tip:</b> One might find many answers for this assignment hidden in the above links!</div>\n",
        "\n",
        "In this section, we will explore the usage of different execution policies offered by the standard and inspect and try to improve their performance using the `nsys profiler`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rqRPAaGn8yZ"
      },
      "source": [
        "We will first implement some trivial workloads using C++11 (& C++17). These will help us understand the maximum amount of effort needed to make our workloads compatible for GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kh9pLgjoUX3"
      },
      "source": [
        "### [std::sort]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv5aysOBtWS8"
      },
      "source": [
        "Look inside the file `sort.cpp`. It contains a very simple example which can be our first candidate made eligible for standard parallelism.\n",
        "\n",
        "Your task is to modify the function `naive_sort` to use [`std::execution`](https://en.cppreference.com/w/cpp/algorithm/execution_policy_tag_t) policies, benchmark and find the execution times and fill in the 2 tables below. To find execution times for smaller workloads, make sure you use a high precision clock. **(Points: 3)**\n",
        "\n",
        "(Feel free to modify the script as you please, to automate the benchmarking process.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHI7CsR2wug6"
      },
      "source": [
        "| Time in ms (average over 1000 iterations) | cpu  | multicore  | gpu  |\n",
        "|:---|:---:|:---:|:---:|\n",
        "| no execution policy  |   |   |   |\n",
        "| std::seq  |   |   |   |\n",
        "|  std::unseq |   |   |   |\n",
        "|  std::par |   |   |   |\n",
        "|  std::par_unseq |   |   |   |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTTkCpFCwuDw"
      },
      "source": [
        "| Time in ms (average over 1000 iterations)\t  |  N=1024 | N=2048  | N=8192  | N=81920  | N=819200  |   |\n",
        "|:---|:---:|:---:|:---:|:---:|:---:|:---:|\n",
        "| cpu  |   |   |   |   |   |   |\n",
        "|  multicore |   |   |   |   |   |   |\n",
        "|  gpu |   |   |   |   |   |   |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HsxluJMyYdE"
      },
      "source": [
        "Mention atleast one important observation from table 1 and table 2. **(Points: 1)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15UFcaeCxlDO",
        "outputId": "0a9ed129-dfad-4996-8810-d1a1f85e650b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"sort_sol.cpp\", line 37: warning: last line of file ends without a newline\n",
            "  }\n",
            "   ^\n",
            "\n",
            "3.21192 ms\n",
            "1 819200\n"
          ]
        }
      ],
      "source": [
        "# your workspace\n",
        "\n",
        "# !nvc++ -Minfo -stdpar=gpu/multicore sort_sol.cpp && ./a.out\n",
        "!nvc++ -Minfo -stdpar=gpu sort_sol.cpp && ./a.out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhHjnT6B0QXn"
      },
      "source": [
        "Your answer:\n",
        "\n",
        "Table 1: For high workloads, the improvement of seq and unseq from cpu to multicore to gpu is marginal compared to the improvement shown by par/par_unseq from cpu to multicore to gpu for large workloads.\n",
        "\n",
        "Table 2: For smaller workloads \"seq\" dominates on the cpu (some reasons being no memory copy from device to host and vice versa, lesser need of parallelism due to smaller workload). However, as the matrix sizes increase, beyond 8192 the best execution time is shown by the GPU when using a parallel (par, par_unseq) execution policy is used. It even performs better than multicore mode, since the number of threads available for parallel execution is much higher in the GPU  than in the all of the CPU cores combined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y4UGXZx0R9I"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwQG7I_80SyY"
      },
      "source": [
        "### Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hT82uLNE0ZT-"
      },
      "source": [
        "Look inside the files `sum.cpp`, `vector_product.cpp`, `swaps.cpp` and `increment.cpp`. It contains naive implementations of some popular algorithms. Your task is to examine the code presented and answer the following questions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBYpMjPM01hN"
      },
      "source": [
        "Using [`std::for_each`](https://en.cppreference.com/w/cpp/algorithm/for_each) only, for each algorithm mention the execution policy that will perform the best? **(Points: 1)**\n",
        "\n",
        "| Algorithm  | Execution policy used |\n",
        "|---|---|\n",
        "| swaps  | seq  |\n",
        "|  naive_sum | seq/unseq (since parallel execution policies will lead to data race)  |\n",
        "|  naive_increment | seq/unseq (higher compute workloads will change this answer to par/par_unseq) |\n",
        "|  naive_inner_prod | seq/unseq (since parallel execution with for_each will cause data race) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA_hnTtM15CS"
      },
      "source": [
        "Port the above \"naive\" implementations to work on GPUs using the following `std` algorithms. **(Points: 4)**\n",
        "\n",
        "| Algorithm  | std:: | function name |\n",
        "|---|---|---| \n",
        "| swaps  |  for_each | std_swaps |\n",
        "|  naive_sum |  reduce | std_reduce_sum |\n",
        "|  naive_increment | transform | std_transform |\n",
        "|  naive_innerp | inner_product | std_innerp |\n",
        "\n",
        "The functions signatures are already provided in each of the code files. Fill up these empty functions and make sure it works with the `main()` provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtHyLM82sSbB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cz4X4fIiyl_B",
        "outputId": "5688ae47-0580-42b1-e4a1-7e80bb1064c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"increment_sol.cpp\", line 69: warning: last line of file ends without a newline\n",
            "  }\n",
            "   ^\n",
            "\n",
            "5\n",
            "0.106532 ms\n",
            "5\n",
            "0.122664 ms\n"
          ]
        }
      ],
      "source": [
        "# your workspace\n",
        "!nvc++ -Minfo increment_sol.cpp && ./a.out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6wxO4-5-bU3"
      },
      "source": [
        "Based on your implementations, answer the following quetions: **(Points: 4)**\n",
        "1. How well does a parallelised `std_swaps` perform against a sequential implementation?\n",
        "2. On which platform (CPU or GPU) does parallelised `std_reduce_sum` perform better?\n",
        "3. For `N=8192` does parallelised `std_tranform` on GPU ever perform better than sequential implementation? If not, explain why or else mention the scenario when it shows improvement over the sequential implementation. (Hint: Adjust the workload assigned within each loop and observe the behavior.) \n",
        "4. Does `std::inner_product` support execution policies? If not, write a new function `std_transform_reduce` which makes use of an algorithm that supports parallel execution policies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CsCtuluyqDF"
      },
      "source": [
        "Your answer:\n",
        "\n",
        "(For detailed reasoning read the comments in solution code files.)\n",
        "\n",
        "1. Poorly as compared to sequential policy.\n",
        "\n",
        "2. On GPUs, only for large workloads (N = 819200, 8192000). For a workload of N=8192 cpu execution with any policy outperforms GPU. It is worth mentioning that in these cases when GPU does not show better performance it is primarily due to poor memory access patterns from device global memory. Using `-stdpar=multicore` is the better option than just relying on a single core CPU execution, since there is no concept of device memory for `multicore`.\n",
        "\n",
        "3. Find the reason in the solution code (`increment_sol.cpp`).\n",
        "\n",
        "4. No, it does not.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Note: On single core CPUs, only concurrent execution is possible hence all policies should show similar execution times with minor differences. This should hold for all the kernels presented in this question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTrYJUPmyniF"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_oHThwrHYly"
      },
      "source": [
        "### Debugging\n",
        "\n",
        "This section will focus on some common pitfalls when using `std::par` with GPUs. \n",
        "\n",
        "To complete this part, you will need to debug, compile and verify the code snippets provided. In the files `debug*.cpp` make minor changes so that it compiles and the checks work. **(Points: 4)**\n",
        "\n",
        "(Hint: Search for a `nvc++` flag which provides additional debug info during compilation.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8qurHBTy25J"
      },
      "outputs": [],
      "source": [
        "# your workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYqp9_lGy4EJ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAnCkrLGU7bH"
      },
      "source": [
        "#Matrix multiplication using std::par"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rlp0qNDNVRCR"
      },
      "source": [
        "Now, you shall finally implement matrix multiplication using std::par. To complete this section, make sure you have the nsys profiler GUI installed and running."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6QLl2aTZAHk"
      },
      "source": [
        "**Part 1**\n",
        "\n",
        "The reference script is provided in `matmul_1.cpp`.\n",
        "\n",
        "The points for this part will depend on the relative performance of your optimised implementation in comparison to the CUDA kernels (under a similar workload).\n",
        "\n",
        "**(Total Points: 6)**\n",
        "\n",
        "*Make sure your implementation uses at max 2 nested `std::for_each`/looping blocks for this part.*\n",
        "\n",
        "Q1. What is the relative performance of your unoptimised implementation when compared to the CUDA kernel? **(Points: 0.5)**\n",
        "\n",
        "Q2. In order to reduce the execution time of your implementation, take the help of the `nsys profiler` to collect logs and identify unnecessary `memcpy` cycles from **device to host** and **host to device**. Attach screenshots clearly marking these memcpy operations from the profiler. **(Points: 0.5)**\n",
        "\n",
        "Q3. Refer to [this](https://developer.nvidia.com/blog/multi-gpu-programming-with-standard-parallel-c-part-2/) article to come up with a fix to the script provided. Explain the fix. **(Points: 0.5)**\n",
        "\n",
        "Q4. Based on the finding in the previous question try to optimise your script as much as possible. Note down the new relative performance of your implementation. **(Points: 3.5)**\n",
        "\n",
        "Q5. In the same article linked above, there is yet another optimisation strategy mentioned which further improves the execution time on top of the previous fix. Identify it and mention the relative speed up from step 4. **(Points: 0.5)**\n",
        "\n",
        "Q6. Based on our observation from the CUDA exercise, is `cudaMallocHost` a better way to allocate memory on the host when using standard parallelism? Explain your answer. **(Points: 0.5)**\n",
        "\n",
        "You will need to submit the `matmul_1.cpp` script provided with your (ultra) optimised implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fjlze69SzThs",
        "outputId": "1bd80449-2bc9-458a-d5a6-c1ca44078a33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"matmul_1.cpp\", line 137: warning: a class type that is not trivially copyable passed through ellipsis\n",
            "      printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d for matmul: %f ms.\\n\\n\", m, k, k, n, elapsed);\n",
            "                                                                                                          ^\n",
            "\n",
            "Time elapsed on matrix multiplication of 1024x1024 . 1024x1024 for matmul: 16.218702 ms.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# your workspace\n",
        "!nvc++ -Minfo -stdpar=gpu matmul_1.cpp && ./a.out\n",
        "\n",
        "# !nsys profile --trace cuda,nvtx -f true -o report2 ./a.out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E7WNSfqzQTw"
      },
      "source": [
        "Your answers:\n",
        "\n",
        "1. ~ (10ms / 20 ms) = 0.5\n",
        "\n",
        "<!-- 2. ![](https://drive.google.com/uc?export=view&id=1sg6hAf8FYFN26ejTxA39_vj3mIeAk1HI) -->\n",
        "\n",
        "2. https://drive.google.com/file/d/1sg6hAf8FYFN26ejTxA39_vj3mIeAk1HI/view?usp=sharing\n",
        "\n",
        "\n",
        "3. The improvement comes from the fact that we are initialising the matrix on the CPU which would need an unnecessary device to host copy since stdpar uses unified memory which will try to get the matrix from device to host resulting in a memory copy.\n",
        "\n",
        "4. (10 ms / 16 ms)\n",
        "\n",
        "5. The second improvement is fby creating GPU pinned memory using cudaMalloc instead of malloc which forces th memory to be available on the GPU. All these optimisations serve on purpose - to reduce te number of memcpy cycles required before we get the actual outputs. (Note: For the actual outputs we will have to incur a device to host memcpy cycle.) - (10 ms / 15 ms)\n",
        "\n",
        "6. cudaMallocHost creates pinned memory explicitly on the CPU and we would have to incur additional host to device and device to host memcpy cycles, which would not be beneficial for our use. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD7XuPW8jIJI"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdWau8RsjJ3h"
      },
      "source": [
        "**Part 2 (Bonus)**\n",
        "\n",
        "The reference script is provided in `matmul_2.cpp`.\n",
        "\n",
        "**(Total Points: 3)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd5vqSE_j588"
      },
      "source": [
        "Q1. In the previous part there was a contraint on the number of nested `std::for_each` blocks that one could use. However, it is more intuitive to represent matrix multiplication in terms of an outer loop on the row index, an intermediate loop on the column index and a inner loop representing the inner product of the row and column. Implement matrix multiplication using atleast 3 nested `std::for_each` blocks. **(Points: 1)**\n",
        "\n",
        "Q2. Did you face any issues in the previous question? What is the best execution time you can obtain using the 3 nested `for_each` blocks? **(Points: 1)**\n",
        "\n",
        "Q3. Can you improve your execution time by switching from row-major to column major for each of the three matrices? How much improvement did you observe? Provide an intuitive explanation for the same. **(Points: 1)**\n",
        "\n",
        "You will need to submit the `matmul_2.cpp` script provided with your nested `for_each` implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6toYTThk0HM_",
        "outputId": "5d913bea-8bc9-4740-80cd-3d0144ccf055"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"matmul_2.cpp\", line 198: warning: a class type that is not trivially copyable passed through ellipsis\n",
            "      printf(\"Time elapsed on matrix multiplication of %dx%d . %dx%d for matmul: %f ms.\\n\\n\", m, k, k, n, elapsed);\n",
            "                                                                                                          ^\n",
            "\n",
            "Time elapsed on matrix multiplication of 1024x1024 . 1024x1024 for matmul: 134.058170 ms.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# your workspace\n",
        "\n",
        "!nvc++ -Minfo -stdpar=gpu matmul_2.cpp && ./a.out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_V2B3nv0Jmg"
      },
      "source": [
        "Your answers:\n",
        "\n",
        "2. Possible issues - cannot create nested std par policies.\n",
        "\n",
        "3. The improvement is because writing (to the output matrix cell) is much more time consuming than reading from the input matrix cells. This is because writing \n",
        "\n",
        "improvement - (134 ms / 1100 ms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOyYq5mil7i3"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment(ans).ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
